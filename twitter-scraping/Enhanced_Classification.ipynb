{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee20974",
   "metadata": {
    "id": "4ee20974"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c522b3b5",
   "metadata": {
    "id": "c522b3b5"
   },
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41aecd1",
   "metadata": {
    "id": "a41aecd1"
   },
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('replyTweets_combined_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b5a2e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6b5a2e6",
    "outputId": "39c37948-6938-4159-aad9-b5d53062a2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 287188 entries, 0 to 287187\n",
      "Data columns (total 23 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   id                   287187 non-null  float64\n",
      " 1   createdAt            287188 non-null  object \n",
      " 2   fullName             287149 non-null  object \n",
      " 3   userName             287188 non-null  object \n",
      " 4   profileImage         287188 non-null  object \n",
      " 5   fullText             287188 non-null  object \n",
      " 6   replyTo              287188 non-null  int64  \n",
      " 7   lang                 287188 non-null  object \n",
      " 8   quoteCount           287188 non-null  int64  \n",
      " 9   retweetCount         287188 non-null  int64  \n",
      " 10  replyCount           287188 non-null  int64  \n",
      " 11  likeCount            287188 non-null  int64  \n",
      " 12  viewCount            103504 non-null  float64\n",
      " 13  sentimentLabel1      1790 non-null    float64\n",
      " 14  sentimentLabel2      1201 non-null    float64\n",
      " 15  sentimentLabel3      996 non-null     float64\n",
      " 16  sentimentLabelFinal  996 non-null     float64\n",
      " 17  cleaned_tweet        287188 non-null  object \n",
      " 18  cleaned_tweet_vader  285637 non-null  object \n",
      " 19  tokens               287188 non-null  object \n",
      " 20  sarcasm              287188 non-null  int64  \n",
      " 21  sentimentDetail      287188 non-null  object \n",
      " 22  sentiment            287188 non-null  object \n",
      "dtypes: float64(6), int64(6), object(11)\n",
      "memory usage: 50.4+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22434af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c22434af",
    "outputId": "42125ef4-a368-4852-f3af-cd90610bdf06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 987 entries, 152428 to 153426\n",
      "Data columns (total 23 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   987 non-null    float64\n",
      " 1   createdAt            987 non-null    object \n",
      " 2   fullName             987 non-null    object \n",
      " 3   userName             987 non-null    object \n",
      " 4   profileImage         987 non-null    object \n",
      " 5   fullText             987 non-null    object \n",
      " 6   replyTo              987 non-null    int64  \n",
      " 7   lang                 987 non-null    object \n",
      " 8   quoteCount           987 non-null    int64  \n",
      " 9   retweetCount         987 non-null    int64  \n",
      " 10  replyCount           987 non-null    int64  \n",
      " 11  likeCount            987 non-null    int64  \n",
      " 12  viewCount            0 non-null      float64\n",
      " 13  sentimentLabel1      987 non-null    float64\n",
      " 14  sentimentLabel2      987 non-null    float64\n",
      " 15  sentimentLabel3      987 non-null    float64\n",
      " 16  sentimentLabelFinal  987 non-null    float64\n",
      " 17  cleaned_tweet        987 non-null    object \n",
      " 18  cleaned_tweet_vader  987 non-null    object \n",
      " 19  tokens               987 non-null    object \n",
      " 20  sarcasm              987 non-null    int64  \n",
      " 21  sentimentDetail      987 non-null    object \n",
      " 22  sentiment            987 non-null    object \n",
      "dtypes: float64(6), int64(6), object(11)\n",
      "memory usage: 185.1+ KB\n"
     ]
    }
   ],
   "source": [
    "labeled_data = combined_df[combined_df['sentimentLabelFinal'].notnull()]\n",
    "labeled_data = labeled_data[labeled_data['cleaned_tweet_vader'].notnull()]\n",
    "labeled_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "-86mg6FNZHSe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-86mg6FNZHSe",
    "outputId": "6229fefa-e1ac-4460-ca25-70ec3dbe7fd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    490\n",
       "0    354\n",
       "2    143\n",
       "Name: sentimentLabelFinal, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data['sentimentLabelFinal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "DAIIKNOf919w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAIIKNOf919w",
    "outputId": "87ec205b-2482-48ea-cb8e-79c32e347fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    490\n",
      "0    490\n",
      "2    490\n",
      "Name: sentimentLabelFinal, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# oversample\n",
    "X = labeled_data[['cleaned_tweet', 'cleaned_tweet_vader', 'sentimentDetail', 'sarcasm']]\n",
    "y = labeled_data['sentimentLabelFinal']\n",
    "\n",
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "resampled_df['sentimentLabelFinal'] = y_resampled\n",
    "\n",
    "print(resampled_df['sentimentLabelFinal'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UZ_9s5dlR4Fe",
   "metadata": {
    "id": "UZ_9s5dlR4Fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "GW6PPDou91e-",
   "metadata": {
    "id": "GW6PPDou91e-"
   },
   "outputs": [],
   "source": [
    "# emotion label mapping to int\n",
    "emotion_label_map = {\"anger\": 0, \"fear\": 1, \"disgust\": 2, \"surprise\": 3, \"joy\": 4, \"neutral\": 5, \"sadness\": 6}\n",
    "resampled_df['emotion_label'] = resampled_df['sentimentDetail'].map(emotion_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "kKqERyrvzW4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKqERyrvzW4d",
    "outputId": "996db0be-5b3a-41f4-fe03-87e6882fe404"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "C:\\Users\\Krithika JK\\AppData\\Local\\Temp\\ipykernel_8276\\1466188277.py:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
      "C:\\Users\\Krithika JK\\AppData\\Local\\Temp\\ipykernel_8276\\1466188277.py:27: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Krithika JK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████████████████████████████████████████████| 37/37 [07:33<00:00, 12.25s/it, training_loss=0.767]\n",
      "Epoch 2/3: 100%|██████████████████████████████████████████████████| 37/37 [07:25<00:00, 12.05s/it, training_loss=0.372]\n",
      "Epoch 3/3: 100%|██████████████████████████████████████████████████| 37/37 [07:13<00:00, 11.72s/it, training_loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       107\n",
      "           1       0.71      0.65      0.68        97\n",
      "           2       0.81      0.96      0.88        90\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.78      0.78      0.78       294\n",
      "weighted avg       0.78      0.78      0.77       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding emotion labels and sarcasm labels as input features to train sentiment classification model\n",
    "\n",
    "train_df, test_df = train_test_split(resampled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Tokenize inputs for both train and test sets\n",
    "train_encodings = tokenizer(list(train_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(test_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Add emotion and sarcasm labels as input features\n",
    "train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
    "train_emotions = torch.tensor(train_df['emotion_label'].tolist(), dtype=torch.long)  # Assuming emotion is encoded as integers\n",
    "train_sarcasms = torch.tensor(train_df['sarcasm'].tolist(), dtype=torch.long)  # Assuming sarcasm is encoded as integers\n",
    "test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
    "test_emotions = torch.tensor(test_df['emotion_label'].tolist(), dtype=torch.long)\n",
    "test_sarcasms = torch.tensor(test_df['sarcasm'].tolist(), dtype=torch.long)\n",
    "\n",
    "# Define TensorDatasets for training and test\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels, train_emotions, train_sarcasms)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels, test_emotions, test_sarcasms)\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Define model architecture\n",
    "class CustomRobertaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(CustomRobertaForSequenceClassification, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 2, num_labels)  # Additional 2 for emotion and sarcasm\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, emotion, sarcasm):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = torch.cat((pooled_output, emotion.unsqueeze(1), sarcasm.unsqueeze(1)), dim=1)  # Concatenate emotion and sarcasm\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = CustomRobertaForSequenceClassification(num_labels=3)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare DataLoader for training and test\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels, emotions, sarcasms = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, emotions, sarcasms)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({'training_loss': train_loss / (step + 1)})\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels, emotions, sarcasms = batch\n",
    "            logits = model(input_ids, attention_mask, emotions, sarcasms)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "  # Print final classification report after all epochs\n",
    "print()\n",
    "print(\"Enhanced model Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ir2mtN32-7yq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ir2mtN32-7yq",
    "outputId": "e1daf4f8-af15-4635-b061-11a1fe588e0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "C:\\Users\\Krithika JK\\AppData\\Local\\Temp\\ipykernel_8276\\3937535269.py:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
      "C:\\Users\\Krithika JK\\AppData\\Local\\Temp\\ipykernel_8276\\3937535269.py:26: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Krithika JK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████████████████████████████████████████████| 37/37 [07:15<00:00, 11.76s/it, training_loss=0.718]\n",
      "Epoch 2/3: 100%|██████████████████████████████████████████████████| 37/37 [07:07<00:00, 11.56s/it, training_loss=0.348]\n",
      "Epoch 3/3: 100%|██████████████████████████████████████████████████| 37/37 [07:03<00:00, 11.44s/it, training_loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sarcasm Enhanced Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76       107\n",
      "           1       0.70      0.64      0.67        97\n",
      "           2       0.87      0.92      0.90        90\n",
      "\n",
      "    accuracy                           0.78       294\n",
      "   macro avg       0.78      0.78      0.78       294\n",
      "weighted avg       0.77      0.78      0.77       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding sarcasm labels as input features to train sentiment classification model\n",
    "\n",
    "train_df, test_df = train_test_split(resampled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Tokenize inputs for both train and test sets\n",
    "train_encodings = tokenizer(list(train_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(test_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Add sarcasm labels as input features\n",
    "train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
    "train_sarcasms = torch.tensor(train_df['sarcasm'].tolist(), dtype=torch.long)  # Assuming sarcasm is encoded as integers\n",
    "test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
    "test_sarcasms = torch.tensor(test_df['sarcasm'].tolist(), dtype=torch.long)\n",
    "\n",
    "# Define TensorDatasets for training and test\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels, train_sarcasms)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels, test_sarcasms)\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Define model architecture\n",
    "class CustomRobertaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(CustomRobertaForSequenceClassification, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 1, num_labels)  # Additional 1 for sarcasm\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sarcasm):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = torch.cat((pooled_output, sarcasm.unsqueeze(1)), dim=1)  # Concatenate sarcasm\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = CustomRobertaForSequenceClassification(num_labels=3)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare DataLoader for training and test\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels, sarcasms = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, sarcasms)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({'training_loss': train_loss / (step + 1)})\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels, sarcasms = batch\n",
    "            logits = model(input_ids, attention_mask, sarcasms)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print final classification report after all epochs\n",
    "print()\n",
    "print(\"Sarcasm Enhanced Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "QYr43SsM-74R",
   "metadata": {
    "id": "QYr43SsM-74R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "C:\\Users\\Krithika JK\\AppData\\Local\\Temp\\ipykernel_8276\\2223678922.py:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
      "C:\\Users\\Krithika JK\\AppData\\Local\\Temp\\ipykernel_8276\\2223678922.py:26: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Krithika JK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|████████████████████████████████████████████████████| 37/37 [06:59<00:00, 11.34s/it, training_loss=0.7]\n",
      "Epoch 2/3: 100%|██████████████████████████████████████████████████| 37/37 [07:02<00:00, 11.42s/it, training_loss=0.388]\n",
      "Epoch 3/3: 100%|██████████████████████████████████████████████████| 37/37 [07:00<00:00, 11.37s/it, training_loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion Enhanced Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77       107\n",
      "           1       0.72      0.53      0.61        97\n",
      "           2       0.79      0.94      0.86        90\n",
      "\n",
      "    accuracy                           0.76       294\n",
      "   macro avg       0.75      0.76      0.75       294\n",
      "weighted avg       0.75      0.76      0.75       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding emotion labels as input features to train sentiment classification model\n",
    "\n",
    "train_df, test_df = train_test_split(resampled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Tokenize inputs for both train and test sets\n",
    "train_encodings = tokenizer(list(train_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(test_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Add emotion labels as input features\n",
    "train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
    "train_emotions = torch.tensor(train_df['emotion_label'].tolist(), dtype=torch.long)  # Assuming emotion is encoded as integers\n",
    "test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
    "test_emotions = torch.tensor(test_df['emotion_label'].tolist(), dtype=torch.long)\n",
    "\n",
    "# Define TensorDatasets for training and test\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels, train_emotions)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels, test_emotions)\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Define model architecture\n",
    "class CustomRobertaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(CustomRobertaForSequenceClassification, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 1, num_labels)  # Additional 1 for emotion\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, emotion):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = torch.cat((pooled_output, emotion.unsqueeze(1)), dim=1)  # Concatenate emotion\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = CustomRobertaForSequenceClassification(num_labels=3)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare DataLoader for training and test\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels, emotions = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, emotions)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({'training_loss': train_loss / (step + 1)})\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels, emotions = batch\n",
    "            logits = model(input_ids, attention_mask, emotions)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print final classification report after all epochs\n",
    "print()\n",
    "print(\"Emotion Enhanced Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vWWgrk-S-76p",
   "metadata": {
    "id": "vWWgrk-S-76p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fRCCNU2u-79Q",
   "metadata": {
    "id": "fRCCNU2u-79Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cgzj-bKn-7_-",
   "metadata": {
    "id": "Cgzj-bKn-7_-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
