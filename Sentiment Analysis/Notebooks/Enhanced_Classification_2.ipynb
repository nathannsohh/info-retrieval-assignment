{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee20974",
      "metadata": {
        "id": "4ee20974"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "from imblearn.over_sampling import RandomOverSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c522b3b5",
      "metadata": {
        "id": "c522b3b5"
      },
      "outputs": [],
      "source": [
        "# Set display options\n",
        "pd.set_option('display.float_format', '{:.0f}'.format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41aecd1",
      "metadata": {
        "id": "a41aecd1"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.read_csv('replyTweets_combined_output_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b5a2e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6b5a2e6",
        "outputId": "7e89d521-0409-4e57-d64f-8f8e7ca31ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 165830 entries, 0 to 165829\n",
            "Data columns (total 23 columns):\n",
            " #   Column               Non-Null Count   Dtype  \n",
            "---  ------               --------------   -----  \n",
            " 0   id                   165830 non-null  int64  \n",
            " 1   createdAt            165830 non-null  object \n",
            " 2   fullName             165807 non-null  object \n",
            " 3   userName             165830 non-null  object \n",
            " 4   profileImage         165830 non-null  object \n",
            " 5   fullText             165830 non-null  object \n",
            " 6   replyTo              165829 non-null  float64\n",
            " 7   lang                 165829 non-null  object \n",
            " 8   quoteCount           165829 non-null  float64\n",
            " 9   retweetCount         165829 non-null  float64\n",
            " 10  replyCount           165829 non-null  float64\n",
            " 11  likeCount            165829 non-null  float64\n",
            " 12  viewCount            103393 non-null  float64\n",
            " 13  sentimentLabel1      1790 non-null    float64\n",
            " 14  sentimentLabel2      1201 non-null    float64\n",
            " 15  sentimentLabel3      996 non-null     float64\n",
            " 16  sentimentLabelFinal  996 non-null     float64\n",
            " 17  cleaned_tweet        165829 non-null  object \n",
            " 18  cleaned_tweet_vader  164894 non-null  object \n",
            " 19  tokens               165829 non-null  object \n",
            " 20  sarcasm              165829 non-null  float64\n",
            " 21  sentimentDetail      165829 non-null  object \n",
            " 22  sentiment            165829 non-null  float64\n",
            "dtypes: float64(12), int64(1), object(10)\n",
            "memory usage: 29.1+ MB\n"
          ]
        }
      ],
      "source": [
        "combined_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84b8b14",
      "metadata": {
        "id": "e84b8b14"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2P3ZXl7RZVIW",
      "metadata": {
        "id": "2P3ZXl7RZVIW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22434af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c22434af",
        "outputId": "6b459dc1-4487-46c7-e9e5-3d79e7db518e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 987 entries, 152428 to 153426\n",
            "Data columns (total 23 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   id                   987 non-null    int64  \n",
            " 1   createdAt            987 non-null    object \n",
            " 2   fullName             987 non-null    object \n",
            " 3   userName             987 non-null    object \n",
            " 4   profileImage         987 non-null    object \n",
            " 5   fullText             987 non-null    object \n",
            " 6   replyTo              987 non-null    float64\n",
            " 7   lang                 987 non-null    object \n",
            " 8   quoteCount           987 non-null    float64\n",
            " 9   retweetCount         987 non-null    float64\n",
            " 10  replyCount           987 non-null    float64\n",
            " 11  likeCount            987 non-null    float64\n",
            " 12  viewCount            0 non-null      float64\n",
            " 13  sentimentLabel1      987 non-null    float64\n",
            " 14  sentimentLabel2      987 non-null    float64\n",
            " 15  sentimentLabel3      987 non-null    float64\n",
            " 16  sentimentLabelFinal  987 non-null    float64\n",
            " 17  cleaned_tweet        987 non-null    object \n",
            " 18  cleaned_tweet_vader  987 non-null    object \n",
            " 19  tokens               987 non-null    object \n",
            " 20  sarcasm              987 non-null    float64\n",
            " 21  sentimentDetail      987 non-null    object \n",
            " 22  sentiment            987 non-null    float64\n",
            "dtypes: float64(12), int64(1), object(10)\n",
            "memory usage: 185.1+ KB\n"
          ]
        }
      ],
      "source": [
        "labeled_data = combined_df[combined_df['sentimentLabelFinal'].notnull()]\n",
        "labeled_data = labeled_data[labeled_data['cleaned_tweet_vader'].notnull()]\n",
        "labeled_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-86mg6FNZHSe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-86mg6FNZHSe",
        "outputId": "1c546285-fab6-4fa0-90e7-e1d8f8547748"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentimentLabelFinal\n",
              "1    490\n",
              "0    354\n",
              "2    143\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "labeled_data['sentimentLabelFinal'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAIIKNOf919w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAIIKNOf919w",
        "outputId": "757b7390-c351-4663-90b7-a6cd0b32a104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentimentLabelFinal\n",
            "1    490\n",
            "0    490\n",
            "2    490\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# oversample\n",
        "X = labeled_data[['cleaned_tweet', 'cleaned_tweet_vader', 'sentimentDetail', 'sarcasm']]\n",
        "y = labeled_data['sentimentLabelFinal']\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "ros = RandomOverSampler()\n",
        "\n",
        "# Perform Random OverSampling\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data to a DataFrame\n",
        "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "resampled_df['sentimentLabelFinal'] = y_resampled\n",
        "\n",
        "# Check the new count of each sentiment label\n",
        "print(resampled_df['sentimentLabelFinal'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UZ_9s5dlR4Fe",
      "metadata": {
        "id": "UZ_9s5dlR4Fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GW6PPDou91e-",
      "metadata": {
        "id": "GW6PPDou91e-"
      },
      "outputs": [],
      "source": [
        "emotion_label_map = {\"anger\": 0, \"fear\": 1, \"disgust\": 2, \"surprise\": 3, \"joy\": 4, \"neutral\": 5, \"sadness\": 6}\n",
        "resampled_df['emotion_label'] = resampled_df['sentimentDetail'].map(emotion_label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kKqERyrvzW4d",
      "metadata": {
        "id": "kKqERyrvzW4d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(resampled_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# Tokenize inputs for both train and test sets\n",
        "train_encodings = tokenizer(list(train_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(list(test_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Add emotion and sarcasm labels as input features\n",
        "train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
        "train_emotions = torch.tensor(train_df['emotion_label'].tolist(), dtype=torch.long)\n",
        "train_sarcasms = torch.tensor(train_df['sarcasm'].tolist(), dtype=torch.long)\n",
        "test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
        "test_emotions = torch.tensor(test_df['emotion_label'].tolist(), dtype=torch.long)\n",
        "test_sarcasms = torch.tensor(test_df['sarcasm'].tolist(), dtype=torch.long)\n",
        "\n",
        "# Define TensorDatasets for training and test\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels, train_emotions, train_sarcasms)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels, test_emotions, test_sarcasms)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# Define model architecture\n",
        "class CustomRobertaForSequenceClassification(nn.Module):\n",
        "    def __init__(self, num_labels=3):\n",
        "        super(CustomRobertaForSequenceClassification, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 2, num_labels)  # Additional 2 for emotion and sarcasm\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emotion, sarcasm):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = torch.cat((pooled_output, emotion.unsqueeze(1), sarcasm.unsqueeze(1)), dim=1)  # Concatenate emotion and sarcasm\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomRobertaForSequenceClassification(num_labels=3)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Prepare DataLoader for training and test\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training loop with progress bar\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
        "    for step, batch in progress_bar:\n",
        "        input_ids, attention_mask, labels, emotions, sarcasms = batch\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask, emotions, sarcasms)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        progress_bar.set_postfix({'training_loss': train_loss / (step + 1)})\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels, emotions, sarcasms = batch\n",
        "            logits = model(input_ids, attention_mask, emotions, sarcasms)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  # Print final classification report after all epochs\n",
        "print()\n",
        "print(\"Enhanced model Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ir2mtN32-7yq",
      "metadata": {
        "id": "ir2mtN32-7yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5549346-0238-4a90-df30-31441fe3e4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/3: 100%|██████████| 37/37 [13:04<00:00, 21.21s/it, training_loss=0.699]\n",
            "Epoch 2/3: 100%|██████████| 37/37 [13:00<00:00, 21.10s/it, training_loss=0.35]\n",
            "Epoch 3/3: 100%|██████████| 37/37 [12:55<00:00, 20.96s/it, training_loss=0.182]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sarcasm Enhanced Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.82      0.77       107\n",
            "           1       0.81      0.56      0.66        97\n",
            "           2       0.86      1.00      0.92        90\n",
            "\n",
            "    accuracy                           0.79       294\n",
            "   macro avg       0.79      0.79      0.78       294\n",
            "weighted avg       0.79      0.79      0.78       294\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(resampled_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# Tokenize inputs for both train and test sets\n",
        "train_encodings = tokenizer(list(train_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(list(test_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Add sarcasm labels as feature\n",
        "train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
        "train_sarcasms = torch.tensor(train_df['sarcasm'].tolist(), dtype=torch.long)  # Assuming sarcasm is encoded as integers\n",
        "test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
        "test_sarcasms = torch.tensor(test_df['sarcasm'].tolist(), dtype=torch.long)\n",
        "\n",
        "# Define TensorDatasets for training and test\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels, train_sarcasms)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels, test_sarcasms)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# Define model architecture\n",
        "class CustomRobertaForSequenceClassification(nn.Module):\n",
        "    def __init__(self, num_labels=3):\n",
        "        super(CustomRobertaForSequenceClassification, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 1, num_labels)  # Additional 1 for sarcasm\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, sarcasm):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = torch.cat((pooled_output, sarcasm.unsqueeze(1)), dim=1)  # Concatenate sarcasm\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomRobertaForSequenceClassification(num_labels=3)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Prepare DataLoader for training and test\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training loop with progress bar\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
        "    for step, batch in progress_bar:\n",
        "        input_ids, attention_mask, labels, sarcasms = batch\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask, sarcasms)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        progress_bar.set_postfix({'training_loss': train_loss / (step + 1)})\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels, sarcasms = batch\n",
        "            logits = model(input_ids, attention_mask, sarcasms)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Print final classification report after all epochs\n",
        "print()\n",
        "print(\"Sarcasm Enhanced Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QYr43SsM-74R",
      "metadata": {
        "id": "QYr43SsM-74R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69b8bce-5c93-4a05-980f-7192e1c8004a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/3: 100%|██████████| 37/37 [13:01<00:00, 21.12s/it, training_loss=0.697]\n",
            "Epoch 2/3: 100%|██████████| 37/37 [13:03<00:00, 21.18s/it, training_loss=0.372]\n",
            "Epoch 3/3: 100%|██████████| 37/37 [12:57<00:00, 21.01s/it, training_loss=0.184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Emotion Enhanced Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.77      0.76       107\n",
            "           1       0.75      0.68      0.71        97\n",
            "           2       0.91      0.98      0.94        90\n",
            "\n",
            "    accuracy                           0.80       294\n",
            "   macro avg       0.80      0.81      0.80       294\n",
            "weighted avg       0.80      0.80      0.80       294\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(resampled_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# Tokenize inputs for both train and test sets\n",
        "train_encodings = tokenizer(list(train_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(list(test_df['cleaned_tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Add emotion labels as feature\n",
        "train_labels = torch.tensor(train_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
        "train_emotions = torch.tensor(train_df['emotion_label'].tolist(), dtype=torch.long)  # Assuming emotion is encoded as integers\n",
        "test_labels = torch.tensor(test_df['sentimentLabelFinal'].tolist(), dtype=torch.long)\n",
        "test_emotions = torch.tensor(test_df['emotion_label'].tolist(), dtype=torch.long)\n",
        "\n",
        "# Define TensorDatasets for training and test\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels, train_emotions)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels, test_emotions)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# Define model architecture\n",
        "class CustomRobertaForSequenceClassification(nn.Module):\n",
        "    def __init__(self, num_labels=3):\n",
        "        super(CustomRobertaForSequenceClassification, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 1, num_labels)  # Additional 1 for emotion\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emotion):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = torch.cat((pooled_output, emotion.unsqueeze(1)), dim=1)  # Concatenate emotion\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomRobertaForSequenceClassification(num_labels=3)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Prepare DataLoader for training and test\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training loop with progress bar\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
        "    for step, batch in progress_bar:\n",
        "        input_ids, attention_mask, labels, emotions = batch\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask, emotions)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        progress_bar.set_postfix({'training_loss': train_loss / (step + 1)})\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels, emotions = batch\n",
        "            logits = model(input_ids, attention_mask, emotions)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Print final classification report after all epochs\n",
        "print()\n",
        "print(\"Emotion Enhanced Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vWWgrk-S-76p",
      "metadata": {
        "id": "vWWgrk-S-76p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fRCCNU2u-79Q",
      "metadata": {
        "id": "fRCCNU2u-79Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cgzj-bKn-7_-",
      "metadata": {
        "id": "Cgzj-bKn-7_-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DdCcHWVX-8Ca",
      "metadata": {
        "id": "DdCcHWVX-8Ca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3wOtxHvH-8Eu",
      "metadata": {
        "id": "3wOtxHvH-8Eu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDmlAO_2-8HK",
      "metadata": {
        "id": "bDmlAO_2-8HK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q6IXfFFG-8J_",
      "metadata": {
        "id": "q6IXfFFG-8J_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9GWRcCrs6QtP",
      "metadata": {
        "id": "9GWRcCrs6QtP"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMDGYYVi6Q9B",
      "metadata": {
        "id": "cMDGYYVi6Q9B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}